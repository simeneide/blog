---
title: Opinion - Bet on the open and global language models
author: Simen Eide
date: 14 September 2023
image: "profile.png"
format:
  html:
    code-fold: true
execute:
  echo: false
---

Sven Thaulow (CTO Schibsted) and me have an [opinion piece in the norwegian paper digi.no](https://www.digi.no/artikler/debatt-bruk-ki-milliarden-riktig-sats-pa-de-apne-globale-sprakmodellene/536777) on open sourced LLM, and the discussion on building from scratch vs using global checkpoints as starting points for Norwegian models.
We argue that we need to bet on pretrained global and open sourced models.

---

Torsdag 7. september annonserte regjeringen at de vil bevilge en milliard kroner over fem år på kunstig intelligens (AI). Språkmodeller har tatt verden med storm, og har mye av æren for dette fokuset. I disse modellene ligger det et stort potensial for innovasjon og verdiskapning i privat og offentlig sektor. I mange tilfeller vil det kreve at vi har gode, oppdaterte språkmodeller som behersker norsk, og at de er tilgjengelige for alle som vil bruke dem.

Det finnes både proprietære og åpne språkmodeller, og store selskaper kappes om å ha den beste proprietære språkmodellen: OpenAI har GPT-4, Google har Bard og Anthropic har Claude. Felles for disse modellene er at de er svært enkle å ta i bruk. Gjennom proprietære apper og APIer, som chatGPT, kan brukere enkelt generere tekst. Brukervennlighet har bidratt mye til modellenes suksess, men betyr også et økt avhengighetsforhold til disse appene. Avhengighet til APIer er en solid forretningsmodell som det er vanskelig for bedriftene å komme seg ut av. I tillegg baserer en slik forretningsmodell seg på at selve modellen (modellvektene) forblir en godt bevart hemmelighet.

Samtidig ser vi en økende trend i etablering og bruk av åpne modeller, der de som lager modellene deler modellvektene og gjør dem tilgjengelige for alle (open source). Åpne modellvekter er for AI det som åpen kildekode er for software - det gjør at alle i prinsippet kan kjøre og kontrollere egen AI-modell, uten å måtte be om lov eller betale for det. Åpne språkmodeller gir oss muligheten til å selv fintrene modellene på våre egne oppgaver og data, og muliggjør videre teknologiutvikling.

OpenAI har hatt et forsprang med sine proprietære GPT-modeller, men de åpne modellene er i ferd med å lukke dette forspranget. De siste modellene ut er Llama2 og Falcon, to åpne modeller som skal kunne [konkurrere med GPT-3.5](https://arxiv.org/pdf/2307.09288.pdf%C3%82%C2%A0) og [GPT-4](https://huggingface.co/blog/falcon-180b) . Et [lekket notat fra Google](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither) argumenterer med at selv ikke de store tech-gigantene kan vinne mot open source-miljøet på sikt. Åpenhet bidrar også til økt transparens og forklarbarhet, fordi man kan undersøke hvorfor modellen gjør som den gjør. Dette kan igjen brukes til å redusere skjevfordelinger og andre uønskede effekter vi ser er tilstede i alle språkmodeller.

Utfordringen med dagens språkmodeller er at de ikke er gode nok på norsk, med mindre de trenes med store mengder norsk tekst. Det er flere initiativer i Norden i dag hvor det bygges språkmodeller fra bunnen av og Schibsted er selv med i [NTNU, UiO og UiS sitt NorGPT prosjekt i NorwAI](https://www.ntnu.edu/norwai). Dette er viktig forskningsarbeid og en av to måter å få etablert fundamentale språkmodeller på norsk eller nordiske språk. Samtidig må vi anerkjenne at det med dagens teknologi er svært kostbart og energikrevende å trene store språkmodeller fra grunnen av, og at mindre aktører ikke vil ha ressurser til dette. Det er også et åpent spørsmål om alle språk er best tjent med å ha sin egen monospråklige modell. Det er flere indikasjoner på at det å trene modeller på mange språk gjør det bedre enn å trene kun på ett språk. For eksempel finnes tale-til-tekst-modellen Whisper (large) kun som flerspråklig, fordi den [slo den monospråklige engelske versjonen](https://cdn.openai.com/papers/whisper.pdf). Derfor må vi også teste ut den andre metoden - å satse på større, åpne fundamentalmodeller og gjøre de gode på norsk, slik at både små og store aktører kan bygge videre på det dyre og nyttige arbeidet open source-miljøet gjør globalt.

Det vil komme en strøm av globale og åpne språkmodeller fremover, og de vil bli bedre og bedre. Vi trenger et miljø i Norge som kan trene de nye modellene videre på norsk tekst og tilgjengeliggjøre dem for alle som vil bruke dem. Dette vil være en viktig investering i fremtiden for norsk språk, teknologi og innovasjon og kan muligens være et fullverdig alternativ til språkmodeller som bygges fra bunnen av. 

