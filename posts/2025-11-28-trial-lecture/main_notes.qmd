---
title: Martingale posteriors in modern AI
subtitle: Trial Lecture, University of Oslo
author: Simen Eide
date: 28 November 2025
format:
  revealjs: 
    theme: simple
    css: custom.css
    #width: 1500
    height: 800
    smaller: true
    min-scale: 1.0
    max-scale: 1.0
    scrollable: false
    fragments: true
    footer: " "
    include-after-body: footer.html
    slide-number: true
created: 2025-11-17T12:32
updated: 2025-11-19T21:29
bibliography: references.bib
#toc: true
#toc-depth: 2
---
# Agenda

Target audience: master students with some knowledge about Bayesian statistics

- Motivational LLM In Context Learning
- Traditional Bayesian review
- Predictive Bayesian introduction
- Predictive resampling algorithm
- Some theory
- Applications
- Conclusion



## LLM In-Context Learning {auto-animate="true"}

:::: {.columns}
::: {.column}
- Assume we ask an LLM to solve the following problem

:::
::: {.column}

```
Where should this person go on holiday based on some information of that person?
```
```
x: Simen just defended his PhD in Machine Learning and enjoys paragliding
y:
```

:::
::::

## LLM In-Context Learning {auto-animate="true"}


:::: {.columns}
::: {.column}
- Assume we ask an LLM to solve the following problem

- [Collect some few shot examples]{style="color: blue;"}

:::
::: {.column}
```
Where should this person go on holiday based on some information of that person?
```
::: {.blue-code}
```
x: Anders is a physicist and likes to discuss philosophy
y: destination=Rome

x: Kamilla enjoys skiing and works at the local university
y: destination=Alps
```
:::
```
x: Simen just defended his PhD in Machine Learning and enjoys paragliding
y:
```

:::
::::

## LLM In-Context Learning {auto-animate="true"}


:::: {.columns}

::: {.column}
- Assume we ask an LLM to solve the following problem
- [Collect some few shot examples]{style="color: blue;"}
- [Let the LLM generate additional examples]{style="color: red;"}


::: {.fragment}
- Repeat and record answer
:::

::: {.fragment}
![](attachments/predictive_resampling_llm_B20_N3.png)
:::



:::

::: {.column}
```
Where should this person go on holiday based on some information of that person?
```
::: {.blue-code}
```
x: Anders is a physicist and likes to discuss philosophy
y: destination=Rome

x: Kamilla enjoys skiing and works at the local university
y: destination=Alps
```
:::
::: {.red-code}
```
x: Maria is retired and spends her time gardening and traveling
y: destination=Madeira

x: Sven just quit his job and is now playing in a band with his friends.
y: destination=Nashville

x: Elias is in military conscription and is considering studying engineering afterward
y: destination=Berlin

x: Ingrid is a medical resident finishing her fourth year of specialty training
y: destination=The Well

x: Thomas just became a partner at a consulting firm and enjoys sailing
y: destination=Maldives
```
:::
```
x: Simen just defended his PhD in Machine Learning and enjoys paragliding
y:
```

:::
::::

::: {.fragment}
**Valid? Posterior predictive?**
:::

# Traditional Bayesian approach

## Traditional Bayesian	approach

::: {.notes}
- Collect data points from an unknown distribution $F_0$ 
- Define a prior $\pi(\theta)$ and a likelihood $f_\theta(y)$
- Compute the posterior $P(\theta | y_{1:n})$
- Compute the posterior predictive $P(y | y_{1:n})$
:::


::: {.incremental}
- We have **collected $y_{1:n}$ data points** from an unknown distribution $F_0$.
- Assume $F_0$ has a true density $y_i \sim f_{\theta_0}$ given by a true parameter $\theta_0$.
- Goal: Find probable values of $\theta_0$ given the data $y_{1:n}$.
:::

::: {.fragment .fade-in-then-semi-out}
- Prior $\pi(\theta)$
- Sampling density $f_{\theta}(y)$
- Likelihood $f_\theta(y) = p(y_{1:n} | \theta) = \prod_{i=1}^n f_\theta(y_i)$

::: {.fragment .fade-in-then-semi-out}
#### Posterior

$$ 
P(\theta | y_{1:n}) = 
	\frac
	{\pi(\theta) p(y_{1:n} | \theta )}
	{\int \pi(\theta) p(y_{1:n} | \theta ) d\theta}
$$ {#eq-posterior}
:::
:::
::: {.fragment .fade-in-then-semi-out}

#### Posterior predictive
We can compute the distribution of a new data point $y$ given the observed data $y_{1:n}$ by using the posterior predictive

$$
P(y | y_{1:n}) = \int f_{\theta}(y) \pi(\theta | y_{1:n}) d\theta
$$ {#eq-posterior_predictive}
:::


## Prior beliefs on Neural Network parameters?!
::: {.notes}
- Neural networks are black box models
- How can we have any prior knowledge about the parameters in hidden layers?
:::

::: {.fragment .fade-in-then-semi-out}
- Neural networks are black box models
- We have no intuition here(!)
:::

::: {.fragment .fade-in-then-semi-out}
#### Standard answer: 

> We dont care, we just want to use it for variability

:::

::: {.fragment .fade-in-then-semi-out}
### Today: Start at a different point
:::

::: {.fragment .fade-in-then-semi-out}
- Instead, define the predictive distribution:

$$P(y_{n+1} | y_{1:n})$$
:::

::: {.fragment .fade-in-then-semi-out}
- More natural to define the predictive distribution instead of prior+likelihood?
:::

# Predictive Bayes Motivation

## Compute statistics from an infinite population
::: {style="font-size: 50%;"}
@holmesStatisticalInferenceExchangeability2023
:::

::: {.fragment .fade-in-then-semi-out}
- Think of bayesian ucertainty to originate from _missing data_:

> The assumption behind most, if not all, statistical problems is that there is an amount of data, $x_{comp}$, which if observed, would yield the problem solved.
:::

::: {.fragment .fade-in}
For example:
:::
::: {.incremental .fade-in-then-semi-out}

- Consider i.i.d. observations from an infinite population.
- We have collected $y_\text{obs} := y_{1:n}$
- The missing data are then $y_\text{mis} := y_{n+1:{\infty}}$
- If we had access to the full data $y_{comp} := \{y_\text{obs}, y_\text{mis} \}$, we could compute any statistics of interest with near zero uncertainty.
:::


## Simulate the missing data
::: {.notes}
- Therefore, we can find a posterior distribution, not by specifying priors and likelihoods, but by specifying the distribution of the missing data given the observed data!
:::
::: {style="font-size: 50%;"}
@holmesStatisticalInferenceExchangeability2023
:::

::: {.fragment}
The Bayesian posterior can be written as

$$
\begin{aligned}
\pi(\theta | y_\text{obs})
=& \int \pi(\theta, y_\text{mis} | y_\text{obs}) dy_\text{mis} \\
=& \int \pi(\theta | y_\text{comp}) P(y_\text{mis} | y_\text{obs}) dy_\text{mis}
\end{aligned}
$$
:::
::: {.fragment .incremental}
- We can make $y_\text{comp}$ arbitrarily large
- Replace the conditional posterior with a point estimate 
$$\pi(\theta | y_\text{comp}) = \delta_{\hat{\theta}(y_\text{comp})}(\theta)$$
- Integrate over the missing data

:::

\

::: {.fragment .fade-in}
Just need to define $P(y_\text{mis} | y_\text{obs})$ ...
:::

# Predictive resampling
::: {style="font-size: 50%;"}
@fongMartingalePosteriorDistributions2022
:::
:::: {.columns}
::: {.column}

Way to sample the missing data given a one step ahead predictive distribution $P(y_{i+1} | y_{1:i})$

::: {.fragment .fade-in-then-semi-out}
$y_{comp} = y_{1:\infty} \approx y_{1:N}$ for some large N.
:::

::: {.fragment .fade-in-then-semi-out}
$$
P(y_{n+1:N} | y_{1:n}) = \prod_{i=n}^N P(y_{i+1} | y_{1:i})
$$
:::
:::

::: {.column}

::: {.fragment .fade-in-then-semi-out}
**Step 1:** Simulate $y_{n+1:\infty}$ by N one step ahead predictions $P(y_{i+1} | y_{1:i})$

:::

::: {.fragment .fade-in-then-semi-out}
**Step 2:** Compute the quantity of interest $\theta(y_{1:N})$ on the full dataset
:::
::: {.fragment .fade-in-then-semi-out}
**Repeat** B times to get a posterior distribution of the quantity of interest
:::

::: {.fragment .fade-in .absolute top=450 right=0}
```python
theta_samples = np.zeros(B)
for b in range(0,B):
	y = np.zeros(N)
	y[:n] = y_obs
	for i in range(n+1,N):
		y[i] = p_i(y[1:(i-1)])

	theta_samples[b] = theta(y)
```
:::
:::
::: {.fragment .fade-in top=600}
**NB: Need to define a valid $P(y_{i+1} | y_{1:i})$**
:::
:::


## Example 1 (problem)
::: {style="font-size: 50%;"}
Adapted example from @fongMartingalePosteriorDistributions2022 \
:::
:::: {.columns}
::: {.column}

::: {.fragment .fade-in-then-semi-out}
Assume we have the model
$$ P(\theta):= \pi(\theta) = N(\theta | 0,1) \\
P(y|\theta):=f_\theta(y) = N(y | \theta, 1)
$$
:::

::: {.fragment .fade-in-then-semi-out}
Conjugate prior gives closed form posterior

$$ P(\theta | y_{1:n}) = N(\theta | \bar{\theta_n}, \bar{\sigma_n}^2 ) $$
where
$$ 
\bar{\theta_n} := \frac{\sum_{i=1}^n y_i}{n+1},
 \bar{\sigma}_n^2 := \frac{1}{n+1} 
$$
:::

::: {.fragment .fade-in-then-semi-out}
and posterior predictive
$$ P(y | y_{1:n}) = N(y | \bar{\theta_n}, \bar{\sigma_n^2} + 1)$$
:::
:::
::: {.column}
::: {.fragment .fade-in-then-semi-out}
Set the true parameter $\theta =2.0$, and then collect $n=50$ values:

```{python}
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

sns.set_theme(style="whitegrid", context="talk", palette="colorblind")

# Parameters
n = 50
theta_true = 2.0
np.random.seed(1)
y_data = np.random.normal(theta_true, 1, size=n)

# Posterior parameters
theta_bar = y_data.sum() / (1 + n)
sigma_bar_std = (1 / (1 + n)) ** 0.5

# Plotting range
x = np.linspace(-1, 4, 1000)

plt.figure(figsize=(6, 6))

# Prior N(0, 1)
plt.plot(x, norm.pdf(x, loc=0, scale=1), label="Prior", linestyle="--")

# Posterior
plt.plot(x, norm.pdf(x, loc=theta_bar, scale=sigma_bar_std), label="Posterior")

# True Parameter
plt.axvline(theta_true, color='black', linestyle=':', label="True Parameter")
plt.legend(fontsize='small')
#plt.title("Prior, Posterior, and True Parameter")
plt.xlabel("Theta")
plt.ylabel("Density")
plt.show()
```
:::
::::

:::
## Example (run predictive resampling)

::: {.notes}
- Assume we made a good guess and think that the posterior predictive is a good function to sample from
- 
:::

:::: {.columns}
::: {.column}

- Let the predictive distribution be the true posterior predictive:
$$ P(y_{i+1} | y_{1:i}) = N(y | \bar{\theta_i}, \bar{\sigma_i^2} + 1)$$

::: {.fragment .fade-in-then-semi-out fragment-index=1}
- **Step 1:**  For $i=n,...,N-1$: \ Sample $y_{i+1} | y_{1:i}$ from the posterior predictive
:::
::: {.fragment .fade-in-then-semi-out fragment-index=2}
- **Step 2:** Compute the point estimate of $\theta$ given the full data $y_{1:N}$:
$$
\hat{\theta}(y_{1:N}) = \frac{\sum_{i=1}^N y_i}{N+1} 
$$
:::
::: {.fragment .fade-in-then-semi-out fragment-index=3}
- **Repeat B times** to get posterior samples of $\theta$
:::
:::
:::
::: {.column}
::: {.r-stack}

::: {.fragment .fade-in  fragment-index=2}
![](attachments/posterior_paths_one_path.png){.absolute top=100 right=-100 height="330"}
:::

::: {.fragment .fade-in fragment-index=3}
![](attachments/posterior_paths.png){.absolute top=100 right=-100 height="330"}
:::

::: {.fragment .fade-in}
![](attachments/combined_n=1.png){.absolute top=100 right=-100 height="660"} 
:::

::: {.fragment .fade-in}
![](attachments/combined_n=100.png){.absolute top=100 right=-100 height="660"}
:::

::: {.fragment .fade-in}
![](attachments/combined_n=200.png){.absolute top=100 right=-100 height="660"}
:::

::: {.fragment .fade-in}
![](attachments/combined_n=500.png){.absolute top=100 right=-100 height="660"}
:::

::: {.fragment .fade-in}
![](attachments/combined_n=999.png){.absolute top=100 right=-100 height="660"}
:::

::: {.fragment .fade-in}
![](attachments/predictive_resampling_animation.gif){.absolute top=430 right=-100 height="360"}
:::

:::

::::

## Why is this useful?

::: {.fragment .fade-in-then-semi-out}
#### Recap
- We can find a posterior statistics $\theta(y_{1:N})$ if we have a valid $P(y_{i+1} | y_{1:i})$.
- Do not need to define prior or likelihood
:::

::: {.fragment .fade-in-then-semi-out}
#### Specification
- Priors on BNN parameters lack a clear interpretation
- Maybe its more intuitive to define a predictive distribution?
- $P(y_{i+1} | y_{1:i})$ feels similar to many black box models we have today
:::

::: {.fragment .fade-in-then-semi-out}
#### Computation
- Predictive resampling instead of MCMC
- Predictive resampling can be parallelized
  - 10-100x speedups
:::

# Theory on the predictive distributions

## What predictive distributions does this work for?
::: {style="font-size: 50%;"}
@fortiniExchangeabilityPredictionPredictive2025
:::

::: {.notes}
We now have a sequence $(Y_n)_{n\ge1}$ where we can generate new samples with the predictive distribution $P_n := P(y_{n+1} | y_{1:n})$.

However, when does this technique give us an actual data distribution $F_0$?
:::

::: {.fragment .fade-in-then-semi-out}
- Have a $P(y_{i+1} | y_{1:i})$ that will generate a sequence $(Y_i)_{i\ge n}$
:::
::: {.fragment .fade-in-then-semi-out}
- When $(Y_i)_{i\ge n}$ give us a data distribution $F_0$? i.e. when does

$$
(Y_n)_{n\ge1} | F_0
$$ exists.
:::

\

::: {.fragment .fade-in-then-semi-out}
In this section we will:

- Introduce a requirement on the sequence $(Y_i)_{i\ge1}$ that makes it possible to define a prior
- Then relax this requirement
- End up with a requirement that $P(y_{i+1} | y_{1:i})$ is a martingale.
:::

## de Finetti's Theorem
::: {.fragment .fade-in-then-semi-out}
::: {#def-exchangeable}
$(Y_i)_{i\ge1}$ is **exchangeable** if 

$$
(Y_{\sigma(1)}, Y_{\sigma(2)},...) \sim (Y_1, Y_2, ..)
$$

for every finite permutation $\sigma$ of $\mathbb{N}$ .
:::
:::

::: {.fragment .fade-in-then-semi-out}
::: {#thm-finetti}
# de Finetti's Theorem
<div style="text-align: center;">

<br> There exist $F_0$ such that $(Y_i) | F_0$ 
<br> if and only if 
<br> $(Y_i)$ is exchangeable.
</div>
:::
:::

\

::: {.fragment .fade-in-then-semi-out}
> If we can construct a $P(y_{i+1} | y_{1:i})$ so that the resulting sequence $(Y_i)_{i\ge n}$ is exchangeable, then we have our familiar traditional Bayes framework
:::

::: {.fragment .fade-in-then-semi-out}
In reality, hard to use exchangeability
:::

## Relaxing exchangeability {auto-animate="true"}

::: {.notes}
c.i.d. : "conditionally on the past, all future observations are identically distributed"
Stationarity: A stationary sequence is a random sequence whose joint probability distribution is invariant over time
:::


::: {.fragment .fade-in-then-semi-out}
::: {#thm-exchangecid-stationary}
# Kallenberg 1988
\
"Exchangeability = Stationarity + conditionally identically distributed"
:::
:::

\

::: {.fragment .fade-in-then-semi-out}
**So let us remove stationarity...**
:::

\
<!-- \

::: {#def-stationarity}

A sequence $(Y_n)_{n\ge 1}$ is **stationary** if 
$$
p_{y_1,..., y_n}(y | y_1,..., y_n) = p_{y_{1+k},..., y_{n+1}}(y | y_1,..., y_n) 
$$
for any $n$ and $k$.
::: -->

\

::: {.fragment .fade-in-then-semi-out}
::: {#def-cid data-id="def-cid"}
# Conditionally identically distributed (c.i.d.)
A sequence $(Y_n)_{n\ge 1}$ is c.i.d. if it satisfies

$$
P(Y_{n+k} | y_1, ..., y_{n}) = P(Y_{n+1} | y_1, .., y_{n})
$$
for all $k\ge1$.
:::

::: {.fragment .fade-in-then-semi-out data-id="def-cid-oneliner"}
> All future observations share the same conditional distribution given the past
:::

:::

## Martingale {auto-animate="true"}

::: {#def-cid-recap data-id="def-cid"}
# Conditionally identically distributed (c.i.d.)

A sequence $(Y_n)_{n\ge 1}$ is c.i.d. if it satisfies

$$
P(Y_{n+k} | y_1, ..., y_{n}) = P(Y_{n+1} | y_1, .., y_{n})
$$
for all $k\ge1$.
:::

::: {data-id="def-cid-oneliner"}
> All future observations share the same conditional distribution given the past
:::

::: {.fragment .fade-in}

This is **equivalent to** saying:

::: {#def-martingale-predictive}
# Martingale of the predictive distribution
$$
E[P(Y_{i+2} \in A | y_{1:{i+1}}) | y_{1:i}] = P(Y_{i+1} \in A | y_{1:i})
$$
for all $A$ and all $i$.
:::
:::

::: {.fragment}
#### Still have many desirable properties

::: {.columns }
::: {.column}
  - $(Y_i)_{i\ge n}$ converge to a distribution $F_0$, 
  - $(Y_i)_{i\ge n}$ are identically distributed, 
:::
::: {.column}
  - $(Y_i)_{i\ge n}$ are **asymptotically** exchangeable
  - The empirical distribution of $(Y_i)_{i\ge n}$ converges to $F_0$
:::
:::
:::

<!-- 
## Martingale posteriors

::: {#def-martingale-posterior}
# Martingale posterior
- Observed $y_{1:n}$ from an unknown distribution $F_0$.
- Interested in an estimate $\theta_\infty = \theta(y_{1:\infty})$ i.e. something that is a function of the full dataset.
- Have a sequence $(Y_n)_{n\ge 1}$ that is c.i.d. and converges to $F_0$.

Then the martingale posterior distribution is defined as

$$
\Pi(\theta_{\infty} \in A | y_{1:n}) := \int \delta \{\theta(y_{1:\infty}) \in A \} dP(Y_{1:\infty} | y_{1:n}) \\
\approx \frac{1}{B} \sum_{b=1}^B \delta \{\theta(y_{1:N}) \in A \}
$$

::: -->

# Applications

## Example 1: Empirical predictive

::: {.fragment .fade-in-then-semi-out}
- Remember example:
  - $Y_i \sim N(\theta, 1)$
  - $\theta=2$
:::

::: {.fragment .fade-in-then-semi-out}
- If we instead of the _true_ posterior predictive assume an empirical predictive on the collected data $y_{1:n}$:


::: {.columns}
::: {.column}
![](attachments/posterior_distr_n=999.png)
:::
::: {.column}
![](attachments/posterior_distr_empirical.png)
:::
:::
:::

::: {.fragment .fade-in-then-semi-out}
- Works but "worse model"
:::

## Are LLMs martingales?

::: {.columns}
::: {.column}

![](attachments/predictive_resampling_llm_B20_N3.png){width=100%}

::: {.fragment .fade-in-then-semi-out}
- Check whether $P(y_{i+1}|y_{1:i})$ is a martingale
:::

::: {.fragment .fade-in-then-semi-out}
- Note: $y_{i+1}$ is not the next token!
:::

:::

::: {.column}
```
Where should this person go on holiday based on some information of that person?
```
::: {.blue-code}
```
x: Anders is a physicist and likes to discuss philosophy
y: destination=Rome

x: Kamilla enjoys skiing and works at the local university
y: destination=Alps
```
:::
::: {.red-code}
```
x: Maria is retired and spends her time gardening and traveling
y: destination=Madeira

x: Sven just quit his job and is now playing in a band with his friends.
y: destination=Nashville

x: Elias is in military conscription and is considering studying engineering afterward
y: destination=Berlin

x: Ingrid is a medical resident finishing her fourth year of specialty training
y: destination=The Well

x: Thomas just became a partner at a consulting firm and enjoys sailing
y: destination=Maldives
```
:::
```
x: Simen just defended his PhD in Machine Learning and enjoys paragliding
y:
```

:::
::::

## Are LLMs martingales?
::: {style="font-size: 50%;"}
@falckAreLargeLanguage2024
:::

::: {.columns}
::: {.column}

- @falckAreLargeLanguage2024 studies martingale property of LLMs in-context learning empirically

::: {.fragment .fade-in-then-semi-out fragment-index=1}
- Tests three LLMs: 
  - `gpt-3.5`,
  - `llama-2-7b`
  - `mistral-7b`
:::

::: {.fragment .fade-in-then-semi-out fragment-index=2}
- Tests three datasets: 
  - Bernoulli, 
  - Gaussian,
  - Synthetic natural language dataset
:::


::: {.fragment .fade-in fragment-index=3}
- **Result: No.**
:::


:::

::: {.column}
::: {.fragment fragment-index=3}
![](attachments/llm-statistics-drift.png){width=100%}
:::

::: {.fragment .fade-in fragment-index=5}
- The expected probability drifts as $i$ increases
:::


::: {.fragment .fade-in fragment-index=6}
- Suggests tools or fine tuning to make LLMs more like martingales
:::

:::
:::

## Tabular Prior Fitted Networks (TabPFN)
::: {style="font-size: 50%;"}
@hollmannTabPFNTransformerThat2023, @naglerUncertaintyQuantificationPriorData2025, @ngTabMGPMartingalePosterior2025
:::
::: {.notes}
Why does the dirichlet mixture help?
:::

::: {.fragment .fade-in-then-semi-out}
- TabPFN: pretrained transformer on tabular data (@hollmannTabPFNTransformerThat2023)
:::

::: {.fragment .fade-in-then-semi-out}
- Observe the same non-martingale property in TabPFN (@naglerUncertaintyQuantificationPriorData2025)
:::

\

::: {.fragment .fade-in-then-semi-out}
### Two different approaches:
:::
\

::: {.columns}
::: {.column}
::: {.fragment .fade-in-then-semi-out}
#### @naglerUncertaintyQuantificationPriorData2025
Only use tabPFN on the first step $P(y_{n+1} | x_{n+1}, z_{1:n})$, then a Dirichlet process mixture for the rest.
:::
:::

::: {.column}
::: {.fragment .fade-in-then-semi-out}
#### @ngTabMGPMartingalePosterior2025
"Ignores" theoretical requirements, checks convergence empirically and says its "good enough".
:::

:::

:::

# Conclusion

## Conclusion

::: {.columns}
::: {.column}
::: {.incremental}
- Introduced a new, alternative Bayesian framework
- Specify predictive distributions instead of priors and likelihoods
- Hints at being able to insert our favourite black-box auto-regressive model
- Highly parallizeable
- **Limitation**: Difficult/unsure how to specify valid predictive distributions
:::
:::

::: {.column}
::: {.fragment .fade-in}
#### Future (/current) work
:::

::: {.incremental}
- Show that different models (predictive distributions) are martingales
- Relaxation of the martingale posterior (e.g. @battistonBayesianPredictiveInference2025)
- Empirical robustness (e.g. @ngTabMGPMartingalePosterior2025)

:::
::: {.fragment .fade-in}
##### Half baked (LLM)-ideas
:::

::: {.fragment .fade-in}
- "LLMs are not row-invariant due to positional embeddings" 
  @ngTabMGPMartingalePosterior2025
:::
::: {.fragment .fade-in}
  - Can we design new, "local" positional embeddings architectures are row-invariant?
:::
::: {.fragment .fade-in}
- Update parametric models inside predictive resampling. LLM posteriors?
:::
:::

<!-- - Use LLMs as parametric models to update inside predictive resampling?
  - Output: Posterior LLM parameters samples -->

:::
:::

## References

::: {#refs} 
:::


# BONUS

## Parametric models
::: {style="font-size: 50%;"}
@holmesStatisticalInferenceExchangeability2023
:::

::: {.columns}
::: {.column}
- Consider a data model $f(y | \theta_0)$
- Let $\hat{\theta}_n = \theta(y_{1:n})$ be an _unbiased_ MLE estimate of $\theta_0$ based on $y_{1:n}$
- Then we can sample a new point and estimate a new parameter

$$ 
y_{n+1} \sim f(y | \hat{\theta}_n) \\
\hat{\theta}_{n+1} = \theta(y_{1:n+1})
$$



:::
::: {.column}
- Idea: If uncertainty in mle estimate decreases with more data, then we can iteratively update 
- 
$$ 
\theta_{m+1} = \theta_m + \epsilon_m \frac{\partial log f(x_{m+1}|\theta_m)}{\partial \theta_m}
$$
where $\epsilon_m$ functions as our learning rate scheduler.

- "Gradient descent style" updates
- Gives us the "frequentist posterior"
:::
:::