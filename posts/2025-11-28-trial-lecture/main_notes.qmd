---
title: Martingale posteriors in modern AI
subtitle: Trial Lecture, University of Oslo
author: Simen Eide
date: 28 November 2025
format:
  revealjs: 
    theme: simple
    css: custom.css
    #width: 1500
    height: 800
    smaller: true
    min-scale: 1.0
    max-scale: 1.0
    scrollable: false
    fragments: true
created: 2025-11-17T12:32
updated: 2025-11-19T21:29
bibliography: references.bib
#toc: true
#toc-depth: 2
---
## Agenda

- Motivational LLM In Context Learning
- Traditional Bayesian approach
- Predictive Bayesian Motivation
- Predictive resampling algorithm
- Theory
- Applications
- Conclusion

::: footer
Traditional Bayes | Predictive Bayesian | Predictive Resampling | Theory | Applications | Conclusion
:::

# LLM In-Context Learning

## LLM In-Context Learning {auto-animate="true"}

:::: {.columns}
::: {.column}
- Assume we ask an LLM to solve the following problem

:::
::: {.column}

```
Where should this person go on holiday based on some information of that person?
```
```
x: Simen just defended his PhD in Machine Learning and enjoys paragliding
y:
```

:::
::::

## LLM In-Context Learning {auto-animate="true"}


:::: {.columns}
::: {.column}
- Assume we ask an LLM to solve the following problem

- [Collect some few shot examples]{style="color: blue;"}

:::
::: {.column}
```
Where should this person go on holiday based on some information of that person?
```
::: {.blue-code}
```
x: Anders is a physicist and likes to discuss philosophy
y: destination=Rome

x: Kamilla enjoys skiing and works at the local university
y: destination=Alps
```
:::
```
x: Simen just defended his PhD in Machine Learning and enjoys paragliding
y:
```

:::
::::

## LLM In-Context Learning {auto-animate="true"}


:::: {.columns}

::: {.column}
- Assume we ask an LLM to solve the following problem
- [Collect some few shot examples]{style="color: blue;"}
- [Let the LLM generate additional examples]{style="color: red;"}


::: {.fragment}
- Repeat and record answer
:::

::: {.fragment}
![](attachments/predictive_resampling_llm_B20_N3.png)
:::

::: {.fragment}
- **Valid? Posterior predictive?**
:::

:::

::: {.column}
```
Where should this person go on holiday based on some information of that person?
```
::: {.blue-code}
```
x: Anders is a physicist and likes to discuss philosophy
y: destination=Rome

x: Kamilla enjoys skiing and works at the local university
y: destination=Alps
```
:::
::: {.red-code}
```
x: Maria is retired and spends her time gardening and traveling
y: destination=Madeira

x: Sven just quit his job and is now playing in a band with his friends.
y: destination=Nashville

x: Elias is in military conscription and is considering studying engineering afterward
y: destination=Berlin

x: Ingrid is a medical resident finishing her fourth year of specialty training
y: destination=The Well

x: Thomas just became a partner at a consulting firm and enjoys sailing
y: destination=Maldives
```
:::
```
x: Simen just defended his PhD in Machine Learning and enjoys paragliding
y:
```

:::
::::

# Traditional Bayesian approach

## Traditional Bayesian	approach

::: {.notes}
- Collect data points from an unknown distribution $F_0$ 
- Define a prior $\pi(\theta)$ and a likelihood $f_\theta(y)$
- Compute the posterior $P(\theta | y_{1:n})$
- Compute the posterior predictive $P(y | y_{1:n})$
:::


::: {.fragment .fade-in-then-semi-out}
- We have **collected $y_{1:n}$ data points** from an unknown distribution $F_0$.
:::

::: {.fragment .fade-in-then-semi-out}
- Prior $\pi(\theta)$
- Sampling density $f_{\theta}(y)$
- Likelihood $f_\theta(y) = p(y_{1:n} | \theta) = \prod_{i=1}^n f_\theta(y_i)$

::: {.fragment .fade-in-then-semi-out}
#### Posterior

$$ 
P(\theta | y_{1:n}) = 
	\frac
	{\pi(\theta) p(y_{1:n} | \theta )}
	{\int \pi(\theta) p(y_{1:n} | \theta ) d\theta}
$$ {#eq-posterior}
:::
:::
::: {.fragment .fade-in-then-semi-out}

#### Posterior predictive
We can compute the distribution of a new data point $y$ given the observed data $y_{1:n}$ by using the posterior predictive

$$
P(y | y_{1:n}) = \int f_{\theta}(y) \pi(\theta | y_{1:n}) d\theta
$$ {#eq-posterior_predictive}
:::

::: footer
**Traditional Bayes** | Predictive Bayesian | Predictive Resampling | Theory | Applications | Conclusion
:::

## Prior in a Neural Network?!
::: {.notes}
- Neural nets are black box models that are hard to interpret
- How can we have any intuition or prior knowledge about the parameters in hidden layers?
:::

::: {.fragment .fade-in-then-semi-out}
- Complicated black box models
- How can we have any intuition?
:::

::: {.fragment .fade-in-then-semi-out}
#### Standard answer: 

> We dont care, we just want to use it for variability

:::

::: {.fragment .fade-in-then-semi-out}
### Today: Start at a different point?
:::

::: {.fragment .fade-in-then-semi-out}
- These black box neural are closer to defining the predictive distribution 

$$p(y_{n+1} | y_{1:n})$$
:::

::: {.fragment .fade-in-then-semi-out}
- Specify a predictive distribution instead of prior+likelihood
:::

# Predictive Bayes Motivation

## Compute statistics from an infinite population

::: {.fragment .fade-in-then-semi-out}
- Think of bayesian ucertainty to originate from _missing data_:

> The assumption behind most, if not all, statistical problems is that there is an amount of data, $x_{comp}$, which if observed, would yield the problem solved.   (@holmesStatisticalInferenceExchangeability2023)
:::

::: {.fragment .fade-in}
For example:
:::
::: {.incremental .fade-in-then-semi-out}

- Consider i.i.d. observations from an infinite population.
- We have collected $y_\text{obs} := y_{1:n}$
- The missing data are then $y_\text{mis} := y_{n+1:{\infty}}$
- If we had access to the full data $y_{comp} := \{y_\text{obs}, y_\text{mis} \}$, we could compute any statistics of interest with near zero uncertainty.
:::


## Simulate the missing data
::: {.notes}
- Therefore, we can find a posterior distribution, not by specifying priors and likelihoods, but by specifying the distribution of the missing data given the observed data!
:::

::: {.fragment .fade-in}
The Bayesian posterior can be written as

$$
\begin{aligned}
\pi(\theta | y_\text{obs})
=& \int \pi(\theta, y_\text{mis} | y_\text{obs}) dy_\text{mis} \\
=& \int \pi(\theta | y_\text{comp}) P(y_\text{mis} | y_\text{obs}) dy_\text{mis}
\end{aligned}
$$
:::
::: {.fragment .incremental}
- We can make $y_\text{comp}$ arbitrarily large
- Replace the conditional posterior with a point estimate 
$$\pi(\theta | y_\text{comp}) = \delta_{\hat{\theta}(y_\text{comp})}(\theta)$$
- Integrate over the missing data

:::

\

::: {.fragment .fade-in-then-semi-out}
**Just need to find $P(y_\text{mis} | y_\text{obs})$...**
:::

## Why is this useful?

- Priors on BNN parameters lack a clear interpretation
- $P(y_{n+1} | y_{1:n})$ are very similar to many black box models we have today
- Resulting method can be much faster than MCMC.

# Predctive resampling
Predictive resampling (@fongMartingalePosteriorDistributions2022) is a way to sample the missing data given $p(y_{n+1} | y_{1:n})$

:::: {.columns}
::: {.column}
::: {.fragment .fade-in-then-semi-out}
1. Simulates $y_{n+1:\infty}$ by N one step ahead predictions from a valid predictive distribution $P(y_{i} | y_{1:i-1})$:

$$
P(y_{n+1:\infty} | y_{1:n}) 
= \prod_{i=n+1}^\infty P(y_i | y_{1:i-1})
\approx \prod_{i=n+1}^N P(y_i | y_{1:i-1})
$$ 
:::

::: {.fragment .fade-in-then-semi-out}
2. Compute the quantity of interest $\theta(y_{1:N})$ on the full dataset
3. Monte Carlo integration: Repeat this B times to get a posterior distribution of the quantity of interest
:::
:::

::: {.column}
::: {.fragment .fade-in .absolute top=350 right=0}
```
theta = np.zeros(B)
for b in range(0,B):
	y = np.zeros(N)
	y[:n] = y_obs
	for i in range(n+1,N):
		y[i] = p_i(y[1:(i-1)])

	theta[b] = theta(y)
```
:::
:::
:::
<!-- 
- Given
	- A collected dataset $y_{1:n}$
	- a valid $p(y_{n+1} | y_1:n)$  
	- A quantity of interest that can be computed if we have the full dataset $\theta(y_{1:\infty})$

Then

- for b in 1:B 
	- for i in $n+1:N$  
		- sample $y_i ~ p(y_{n+1} | y_1:n)$  
	- compute $\theta_b = \theta(y_{1:\infty})$  
	
- Then $\{\theta_b\}_{b=1}^N$ are posterior draws of our quantity of interest! 
-->

::: {.fragment .fade-in-then-semi-out}
**NB: We still have not defined any $p(y_{n+1} | y_{1:n})$ we can use!**
:::

## Example 1 (problem)
::: footer
Adapted example from @fongMartingalePosteriorDistributions2022 \
Traditional Bayes | **Predictive Bayesian** | Predictive Resampling | Examples | Conclusion
:::

:::: {.columns}
::: {.column}

::: {.fragment .fade-in-then-semi-out}
Assume we have the model
$$ P(\theta):= \pi(\theta) = N(\theta | 0,1) \\
P(y|\theta):=f_\theta(y) = N(y | \theta, 1)
$$
:::

::: {.fragment .fade-in-then-semi-out}
Conjugate prior gives closed form posterior

$$ P(\theta | y_{1:n}) = N(\theta | \bar{\theta_n}, \bar{\sigma_n^2} ) $$
where
$$ 
\bar{\theta_n} := \frac{\sum_{i=1}^n y_i}{n+1},
 \bar{\sigma_n^2} := \frac{1}{n+1} 
$$
:::

::: {.fragment .fade-in-then-semi-out}
and posterior predictive
$$ P(y | y_{1:n}) = N(y | \bar{\theta_n}, \bar{\sigma_n^2} + 1)$$
:::
:::
::: {.column}
::: {.fragment .fade-in-then-semi-out}
Set the true parameter $\theta =2.0$, and then collect $n=50$ values:

```{python}
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

sns.set_theme(style="whitegrid", context="talk", palette="colorblind")

# Parameters
n = 50
theta_true = 2.0
np.random.seed(1)
y_data = np.random.normal(theta_true, 1, size=n)

# Posterior parameters
theta_bar = y_data.sum() / (1 + n)
sigma_bar_std = (1 / (1 + n)) ** 0.5

# Plotting range
x = np.linspace(-1, 4, 1000)

plt.figure(figsize=(6, 6))

# Prior N(0, 1)
plt.plot(x, norm.pdf(x, loc=0, scale=1), label="Prior", linestyle="--")

# Posterior
plt.plot(x, norm.pdf(x, loc=theta_bar, scale=sigma_bar_std), label="Posterior")

# True Parameter
plt.axvline(theta_true, color='black', linestyle=':', label="True Parameter")
plt.legend(fontsize='small')
#plt.title("Prior, Posterior, and True Parameter")
plt.xlabel("Theta")
plt.ylabel("Density")
plt.show()
```
:::
::::

:::
## Example (run predictive resampling)

::: {.notes}
We are allowed to use the posterior predictive as our one step ahead predictive distribution. Not only are we allowed, but we will get the exact same posterior as traditional bayes!
:::

:::: {.columns}
::: {.column}

- Let the predictive distribution be the true posterior predictive:
$$ P(y_{n+1} | y_{1:n}) = N(y | \bar{\theta_n}, \bar{\sigma_n^2} + 1)$$



::: {.incremental}
- **Step 1:** Sample $y_{n+1:N}$ from the posterior predictive $P(y | y_{1:n})$  
- **Step 2:** Compute the point estimate of $\theta$ given the full data $y_{1:N}$:
$$ \hat{\theta}(y_{1:N}) = \frac{\sum_{i=1}^N y_i}{N+1} $$
- **Step 3:** Repeat B times to get posterior samples of $\theta$
:::
:::
::: {.column}
::: {.r-stack}

::: {.fragment .fade-in}
![](attachments/posterior_paths.png){.absolute top=100 right=-100 height="330"}
:::

::: {.fragment .fade-in}
![](attachments/predictive_resampling_animation.gif){.absolute top=450 right=-100 height="330"}
:::
:::
:::
::::

# Theory on the predictive distributions

## What predictive distributions does this work for?

We now have a sequence $(Y_n)_{n\ge1}$ where we can generate new samples with the predictive distribution $P_n := P(y_{n+1} | y_{1:n})$.

However, when does this technique give us an actual data distribution $F_0$?
$$
(Y_n)_{n\ge1} | F_0
$$

\

In this section we will:

- Introduce a requirement on the sequence $(Y_n)$ that makes it possible to define a prior
- Then relax this requirement
- End up with a requirement that $P(y_{n+1} | y_{1:n})$ is a martingale.


::: {.notes}
Outline next steps:
- de Finetti explain prior exist iff sequence is exchangeable
  - example: Empirical distribution?
- Exchangeability is a strong assumption, hard to construct in practice

- Relax exchangeability
  - We might not care about excplicitly defining a prior
  - Want to use predictive resampling to be faster than mcmc
  - "How can we relax exchangeability while preserving the idea that observations arise from the same unknown distribution"? (Fortini seminar 25)

- Exchangeability = Stationarity + c.i.d.
	- What happens if we drop stationarity?
	- This is equivalent to saying that $P_n$ is a martingale. @fortiniTheoreticalFoundationsPredictive
		- through 
:::

## de Finetti's Theorem

::: {#def-exchangeable}
$(Y_n)$ is **exchangeable** if 

$$
(Y_{\sigma(1)}, Y_{\sigma(2)},...) \sim (Y_1, Y_2, ..)
$$

for every finite permutation $\sigma$ of $\mathbb{N}$ .
:::

::: {#thm-finetti}
# de Finetti's Theorem
<div style="text-align: center;">

<br> There exist $F_0$ such that $(Y_n) | F_0 \sim F^\infty$ 
<br> if and only if 
<br> $(Y_n)$ is exchangeable.
</div>

:::

\

> If we can construct a $P_n$ so that the resulting sequence $(Y_n)$ is exchangeable, then we have our familiar traditional Bayes framework

In reality, hard to use exchangeability

## Relaxing exchangeability {auto-animate="true"}

::: {.notes}
c.i.d. : "conditionally on the past, all future observations are identically distributed"
Stationarity: A stationary sequence is a random sequence whose joint probability distribution is invariant over time
:::

::: {#thm-exchangecid-stationary}
# Kallenberg 1988

"Exchangeability = Stationarity + c.i.d."
:::

\

::: {#def-stationarity}

A sequence $(Y_n)_{n\ge 1}$ is **stationary** if 
$$
p_{y_1,..., y_n}(y | y_1,..., y_n) = p_{y_{1+k},..., y_{n+1}}(y | y_1,..., y_n) 
$$
for any $n$ and $k$.
:::

\

::: {#def-cid}
# Conditionally identically distributed (c.i.d.)
A sequence $(Y_n)_{n\ge 1}$ is c.i.d. if it satisfies

$$
p_{y_{n+k}}(y | y_1, ..., y_{n-1}) = p_{y_n}(y| y_1, .., y_{n-1})
$$
for all $k\ge1$.
:::

**So let us remove stationarity...**


## Martingale {auto-animate="true"}

::: {.fragment .fade-in-then-semi-out}
::: {#def-cid-recap}
# Conditionally identically distributed (c.i.d.)

A sequence $(Y_n)_{n\ge 1}$ is c.i.d. if it satisfies

$$
p_{y_{n+k}}(y | y_1, ..., y_{n-1}) = p_{y_n}(y| y_1, .., y_{n-1})
$$
for all $k\ge1$.
:::
:::

::: {.fragment .fade-in-then-semi-out}
> All future observations share the same conditional distribution given the past
:::

::: {.fragment .fade-in-then-semi-out}
This is equivalent to saying that the predictive distributions are martingales:

::: {#def-martingale-predictive}
# Martingale of the predictive distribution
$$
E(p_{n+1}(A) | y_{1:n}) = p_n(A)
$$
for all $A$ and all $n$.
:::
:::

::: {.fragment}
- Still have many desirable properties 
  - $(Y_n)$ converge to a distribution $F_0$, 
  - $Y_n$ are identically distributed, 
  - $Y_n$ are **asymptotically** exchangeable
:::

## Martingale posteriors

::: {#def-martingale-posterior}
# Martingale posterior
- Observed $y_{1:n}$ from an unknown distribution $F_0$.
- Interested in an estimate $\theta_\infty = \theta(y_{1:\infty})$ i.e. something that is a function of the full dataset.
- Have a sequence $(Y_n)_{n\ge 1}$ that is c.i.d. and converges to $F_0$.

Then the martingale posterior distribution is defined as

$$
\Pi(\theta_{\infty} \in A | y_{1:n}) := \int \delta \{\theta(y_{1:\infty}) \in A \} dP(Y_{1:\infty} | y_{1:n}) \\
\approx \frac{1}{B} \sum_{b=1}^B \delta \{\theta(y_{1:N}) \in A \}
$$

:::

# Applications
## Parametric models

## Are LLMs martingales?

::: {.columns}
::: {.column}

- Check whether $P(y_{n+1}|y_{1:n})$ is a martingale

- Do we get the same distribution when generating more examples?

![](attachments/predictive_resampling_llm_B20_N3.png)

![](attachments/predictive_resampling_llm_B20_N10.png)

:::

::: {.column}
```
Where should this person go on holiday based on some information of that person?
```
::: {.blue-code}
```
x: Anders is a physicist and likes to discuss philosophy
y: destination=Rome

x: Kamilla enjoys skiing and works at the local university
y: destination=Alps
```
:::
::: {.red-code}
```
x: Maria is retired and spends her time gardening and traveling
y: destination=Madeira

x: Sven just quit his job and is now playing in a band with his friends.
y: destination=Nashville

x: Elias is in military conscription and is considering studying engineering afterward
y: destination=Berlin

x: Ingrid is a medical resident finishing her fourth year of specialty training
y: destination=The Well

x: Thomas just became a partner at a consulting firm and enjoys sailing
y: destination=Maldives
```
:::
```
x: Simen just defended his PhD in Machine Learning and enjoys paragliding
y:
```

:::
::::

## Are LLMs martingales? (Falck et al 2024)
- @falckAreLargeLanguage2024 studies whether LLMs obey the martingale property for in context learning
- Tests three LLMs: `gpt-3.5`, `llama-2-7b` and `mistral-7b`
- Tests three datasets: Bernoulli, Gaussian and a synthetic natural language dataset

![](attachments/20251121161334.png)

- Suggests tools or fine tuning to make LLMs more like martingales

## TabPFN

::: {.notes}
Why does the dirichlet mixture help?
:::

::: {.columns}
::: {.column}
::: {.fragment fragment-index=1}
- @naglerUncertaintyQuantificationPriorData2025 observe the same phenomenon in TabPFN
:::
::: {.fragment fragment-index=2}
- TabPFN: tabular $z_i = (x_i,y_i)$ trained on synthetic data
:::
::: {.fragment fragment-index=3}
- Inference:
$$
P(y_{n+1} | x_{n+1}, z_{1:n})
$$
- Use a Dirichlet process mixture to construct posterior samples
- Quantile regression on tabular tasks
:::

::: {.fragment fragment-index=4}
- Faster, smaller Credible intervals
:::

:::

::: {.column}
::: {.r-stack}
::: {.fragment fragment-index=1}
![](attachments/20251122170556.png){.absolute top=0 right=0 height="330"}
:::
::: {.fragment}
![](attachments/20251122171844.png){.absolute top=350 right=0 height="400"}
:::
:::
:::

:::
# Conclusions

## Conclusion

## Future

## References

::: {#refs} 